                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚     User (You)     â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Web UI (Streamlit App)  â”‚
                                    â”‚   [web/app.py]           â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚  User input
                                              â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚     ğŸ¤– Bot Logic (query_engine.py)   â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                                 â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Cypher Generator â”‚â—„â”€â”€â”€â”€â”€â”¤ If Graph Question Detected  â”‚â”€â”€â”€â”€â–¶ Generates Cypher Query     â”‚
â”‚  [llm_cypher_generator.py]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚
              â–¼                                                            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   LLM (Ollama) â”‚                                           â”‚  Knowledge Graph DB â”‚
     â”‚  llama3 model  â”‚                                           â”‚     [Neo4j]         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                               â”‚
                                                                               â–¼
                                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                             â”‚    Graph Query Result  â”‚
                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                               â”‚
                                                                               â–¼
                                                      â†â”€â”€â”€â”€â”€â”€ Result returned to Bot â”€â”€â”€â”€â”€â”€â†’

       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ If Graph route fails or   â”‚
       â”‚ Not enough info in Graph  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Vector DB (FAISS)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                   â–¼                             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
      â”‚ Semantic Search Results    â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                 â–¼                               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
      â”‚ Answer Generator (LLM)     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ [vector_llm_answer.py]     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
       â†â”€â”€â”€â”€â”€â”€ Final Answer to Bot

                     â–¼
        â†â”€â”€â”€â”€ Return Answer to UI â†â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ğŸ›  ETL BACKGROUND FLOW (once or scheduled ingest)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Jira API         â”‚
         â”‚ (Data Center/Cloud)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ ETL Script [load_jira.py]  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Insert into Vector DB  â”‚       â”‚ Insert into Graph DB (Neo4j)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         ğŸ§± Infra/Deployment Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    - Ollama runs in Docker container (port 11434)
    - Neo4j runs in container (bolt://7687, HTTP 7474)
    - Streamlit UI can run in container (port 8501)
    - Shared volume or mounted model (for Ollama)


Tools Involved
Component	Technology
Web UI	Streamlit
Bot logic	Python
LLM	LLaMA3 via Ollama
Vector DB	FAISS / Chroma
Graph DB	Neo4j
ETL	Jira API + Python
Dockerized?	Yes (optional)


import asyncio
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

async def test():
    url = "http://localhost:9000/mcp"
    async with streamablehttp_client(url) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            resp = await session.list_tools()
            print("Tools:", [t.name for t in resp.tools])

asyncio.run(test())





import asyncio
import json
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

MCP_URL = "http://localhost:9000/mcp"
ISSUE_KEY = "DDLT-111"

async def main():
    async with streamablehttp_client(MCP_URL) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            result = await session.call_tool(
                "jiraGetIssue",
                arguments={"key": ISSUE_KEY}
            )

            print("\n=== RAW CONTENT BLOCKS ===")
            for block in result.content:
                print(block)

            print("\n=== PARSED ISSUE JSON ===")
            for block in result.content:
                if hasattr(block, "text"):
                    try:
                        issue = json.loads(block.text)
                        print(json.dumps(issue, indent=2))
                    except Exception:
                        print("Not JSON block:", block.text)

asyncio.run(main())


import asyncio
import json
import requests
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

# ===========================
# CONFIG
# ===========================
MCP_URL = "http://localhost:9000/mcp"
LLAMA_URL = "http://localhost:11434/api/chat"     # Ollama endpoint

# ===========================
# CALL LLaMA (step 1)
# ===========================
def call_llama(messages):
    payload = {
        "model": "llama3.1",
        "messages": messages,
        "stream": False
    }

    r = requests.post(LLAMA_URL, json=payload)
    r.raise_for_status()
    return r.json()


# ===========================
# HANDLE TOOL CALLS
# ===========================
async def call_mcp_tool(tool_name, arguments):
    async with streamablehttp_client(MCP_URL) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool(tool_name, arguments)

            # parse MCP text blocks
            for block in result.content:
                if hasattr(block, "text"):
                    try:
                        return json.loads(block.text)
                    except:
                        return block.text

    return None


# ===========================
# MAIN AGENT LOGIC
# ===========================
async def ask_jira_agent(user_question):
    print(f"\n===== USER ASKS: {user_question} =====\n")

    messages = [
        {
            "role": "system",
            "content": (
                "You are an AI assistant with access to Jira tools. "
                "If user asks about Jira issues, ALWAYS produce tool calls in JSON like:\n"
                "{\"tool\": \"jira_get_issue\", \"arguments\": {\"key\": \"DDLT-111\"}}\n"
            )
        },
        {
            "role": "user",
            "content": user_question
        }
    ]

    # Step 1 â€” Ask LLaMA what tool to call
    llama_res = call_llama(messages)
    assistant_msg = llama_res["message"]

    print("STEP 1 â€” LLaMA response:")
    print(assistant_msg)

    # Step 2 â€” Detect tool call
    tool_calls = assistant_msg.get("tool_calls", [])

    if not tool_calls:
        print("âŒ No tool calls detected.")
        return assistant_msg["content"]

    final_answer = None

    # handle all tool calls
    for tc in tool_calls:
        tool_name = tc["tool"]
        arguments = tc["arguments"]

        print(f"\nSTEP 2 â€” Calling tool: {tool_name} with args = {arguments}")

        tool_result = await call_mcp_tool(tool_name, arguments)

        print("\nSTEP 3 â€” MCP TOOL RESULT:")
        print(json.dumps(tool_result, indent=2))

        # Step 4 â€” Feed tool result back to LLaMA for final answer
        followup_messages = [
            *messages,
            assistant_msg,
            {
                "role": "tool",
                "content": json.dumps(tool_result),
                "name": tool_name
            }
        ]

        final_llama = call_llama(followup_messages)
        final_answer = final_llama["message"]["content"]

    print("\n===== FINAL ANSWER FROM LLaMA =====")
    print(final_answer)

    return final_answer


# ===========================
# CLI EXAMPLE
# ===========================
if __name__ == "__main__":
    question = input("Ask Jira Agent: ")
    asyncio.run(ask_jira_agent(question))




import asyncio
import json
import requests
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

LLAMA_URL = "http://localhost:1234/v1/completions"
MCP_URL = "http://localhost:9000/mcp"


# === LLaMA call using only prompt ===
def llama_generate(prompt):
    payload = {
        "prompt": prompt,
        "max_tokens": 512,
        "temperature": 0
    }
    r = requests.post(LLAMA_URL, json=payload)
    r.raise_for_status()
    return r.json()["text"]


# === CALL MCP TOOL ===
async def call_mcp_tool(tool_name, arguments):
    async with streamablehttp_client(MCP_URL) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool(tool_name, arguments)

            for block in result.content:
                if hasattr(block, "text"):
                    try:
                        return json.loads(block.text)
                    except:
                        return block.text
    return None


# === MAIN AGENT ===
async def ask_jira_agent(user_question):

    # SYSTEM INSTRUCTIONS for tool calling
    system_prompt = """
You are a Jira assistant. 
When you need to call a tool, output ONLY this format:

<tool_call>
{"tool": "jira_get_issue", "arguments": {"key": "DDLT-111"}}
</tool_call>

After receiving tool results, I will send them back to you.
Then you respond with a natural language explanation.
"""

    # Ask LLaMA what to do
    initial_prompt = system_prompt + "\nUser: " + user_question
    response = llama_generate(initial_prompt)

    print("\n=== MODEL RAW RESPONSE ===")
    print(response)

    # Detect <tool_call>
    if "<tool_call>" not in response:
        print("NO TOOL CALL â†’ FINAL ANSWER:")
        return response

    json_str = response.split("<tool_call>")[1].split("</tool_call>")[0].strip()
    tool_call = json.loads(json_str)

    tool_name = tool_call["tool"]
    arguments = tool_call["arguments"]

    print(f"\nDetected tool call: {tool_name} {arguments}")

    # Call MCP tool
    result = await call_mcp_tool(tool_name, arguments)

    print("\n=== MCP RESULT ===")
    print(result)

    # Final prompt to LLaMA to explain result
    final_prompt = f"""
System: Here are the Jira results from the tool call:
{json.dumps(result)}

Explain to the user in clear English or Vietnamese depending on question.
"""

    final_answer = llama_generate(final_prompt)

    print("\n=== FINAL ANSWER ===")
    print(final_answer)

    return final_answer


# CLI
if __name__ == "__main__":
    q = input("Ask Jira Agent: ")
    asyncio.run(ask_jira_agent(q))


import asyncio
import json
import re
import requests
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

# ===========================
# CONFIG
# ===========================
LLAMA_URL = "https://your-llama4-server.com/v1/text/generate"
MCP_URL = "http://localhost:9000/mcp"


# ===========================
# LLaMA4 GENERATE API
# ===========================
def llama_generate(prompt):
    resp = requests.post(
        LLAMA_URL,
        json={
            "prompt": prompt,
            "temperature": 0,
            "max_tokens": 512
        }
    )
    resp.raise_for_status()
    return resp.json()["text"]


# ===========================
# MCP TOOL CALL
# ===========================
async def call_mcp_tool(tool_name, arguments):
    async with streamablehttp_client(MCP_URL) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            result = await session.call_tool(tool_name, arguments)

            for block in result.content:
                if hasattr(block, "text"):
                    try:
                        return json.loads(block.text)
                    except:
                        return block.text
    return None


# ===========================
# TOOL CALL JSON PARSER
# ===========================
def extract_tool_call(response_text):
    print("\nRAW RESPONSE:", repr(response_text))

    match = re.search(r"<tool_call>(.*?)</tool_call>", response_text, re.DOTALL)
    if not match:
        return None

    json_str = match.group(1).strip()

    # Clean invisible chars
    json_str = "".join(ch for ch in json_str if ord(ch) < 128)

    # Remove line breaks
    json_str = json_str.replace("\n", "").replace("\r", "").strip()

    return json.loads(json_str)


# ===========================
# MAIN AGENT LOGIC
# ===========================
async def ask_jira_agent(question):

    system_prompt = """
You are a Jira assistant.
When the user asks about Jira issues, respond ONLY with this format:

<tool_call>
{"tool": "jira_get_issue", "arguments": {"key": "DDLT-111"}}
</tool_call>

Rules:
- No explanation
- No natural language outside JSON
- JSON must be strict, valid, and use double quotes
"""

    # STEP 1 â€” Ask the model to decide tool call
    prompt = system_prompt + "\nUser: " + question
    response = llama_generate(prompt)

    print("\nSTEP 1 â€” MODEL DECISION:")
    print(response)

    tool_call = extract_tool_call(response)

    if not tool_call:
        print("\nNO TOOL CALL. FINAL ANSWER:")
        return response

    tool_name = tool_call["tool"]
    arguments = tool_call["arguments"]

    print(f"\nSTEP 2 â€” CALLING MCP TOOL {tool_name} with {arguments}")

    tool_result = await call_mcp_tool(tool_name, arguments)

    print("\nSTEP 3 â€” MCP RESULT:")
    print(tool_result)

    # STEP 4 â€” Ask LLaMA4 to generate natural answer
    final_prompt = f"""
You are a Jira assistant. 
Summarize the following Jira issue result in a natural answer:

{json.dumps(tool_result, indent=2)}
"""

    final_answer = llama_generate(final_prompt)

    print("\nFINAL ANSWER:")
    print(final_answer)
    return final_answer


# CLI
if __name__ == "__main__":
    q = input("Ask Jira Agent: ")
    asyncio.run(ask_jira_agent(q))







system_prompt = """
You are a smart Jira assistant with access to multiple tools.

## GOAL
- If the user question REQUIRES live Jira data (issue fields, status, comments, assignee, JQL search, transitions, etc.), you MUST respond ONLY with a <tool_call> block.
- If the question is general knowledge or explanation (e.g., â€œWhat is Jira?â€, â€œHow to write a good bug?â€) and does NOT require live Jira data, then DO NOT use any tool_call. Just answer normally.

## SUPPORTED TOOLS

1) jira_get_issue
   Use when the user asks about ONE specific issue key.
   arguments: {"key": "<ISSUE-KEY>"}

   Use for:
   - status of ABC-123
   - who is assigned to ABC-123
   - show summary / details / description of ABC-123

2) jira_search_jql
   Use when the user wants a LIST of issues or filtered search.
   arguments: {"jql": "<JQL-QUERY>"}

   Examples:
   - all open bugs in project DDLT
   - issues assigned to me today
   - all tickets with status Done this week

3) jira_list_projects
   Use when user asks about available projects.
   arguments: {}

4) jira_get_comments
   Use when user asks to see comments of an issue.
   arguments: {"key": "<ISSUE-KEY>"}

5) jira_add_comment
   Use when user wants to add a comment to an issue.
   arguments: {"key": "<ISSUE-KEY>", "body": "<COMMENT-TEXT>"}

6) jira_get_transitions
   Use when user asks: "what statuses can this issue move to?"
   arguments: {"key": "<ISSUE-KEY>"}

7) jira_transition_issue
   Use when user wants to CHANGE the status / workflow of an issue.
   arguments: {"key": "<ISSUE-KEY>", "transition": "<TARGET-STATUS-NAME>"}

8) jira_assign_issue
   Use when user wants to assign or reassign an issue.
   arguments: {"key": "<ISSUE-KEY>", "assignee": "<USERNAME>"}

## OUTPUT FORMAT

### WHEN YOU DECIDE TO USE A TOOL:
- Output EXACTLY:

<tool_call>
{"tool": "<tool_name_here>", "arguments": { ... }}
</tool_call>

- No explanation.
- No extra text.
- JSON must be strict and valid (double quotes, no trailing commas).

### WHEN YOU DO NOT NEED ANY TOOL:
- Just answer the question in natural language.
- Do NOT output <tool_call> at all.

## FEW-SHOT EXAMPLES

User: what is Jira?
â†’ (No tool_call, just explain what Jira is)

User: check status of DDLT-111
â†’
<tool_call>
{"tool": "jira_get_issue", "arguments": {"key": "DDLT-111"}}
</tool_call>

User: who is assigned to DDLT-222?
â†’
<tool_call>
{"tool": "jira_get_issue", "arguments": {"key": "DDLT-222"}}
</tool_call>

User: list all open bugs in project DDLT
â†’
<tool_call>
{"tool": "jira_search_jql", "arguments": {"jql": "project = DDLT AND issuetype = Bug AND statusCategory != Done"}}
</tool_call>

User: add comment 'Please retest' to DDLT-333
â†’
<tool_call>
{"tool": "jira_add_comment", "arguments": {"key": "DDLT-333", "body": "Please retest"}}
</tool_call>

User: move DDLT-444 to Done
â†’
<tool_call>
{"tool": "jira_transition_issue", "arguments": {"key": "DDLT-444", "transition": "Done"}}
</tool_call>

User: show all jira projects
â†’
<tool_call>
{"tool": "jira_list_projects", "arguments": {}}
</tool_call>

Now handle the next user question accordingly.
"""








JQL_SYSTEM_PROMPT = """
You are a Jira JQL generator.
Your job is to convert a natural language request into ONE valid Jira JQL query.

RULES:
- Output ONLY the JQL string. No explanation, no quotes around the whole query.
- Do NOT wrap in backticks.
- Use double quotes for all string values.
- If a project key is known, always include: project = <KEY>
- For "open" issues, use: statusCategory != Done
- For "bugs only", use: issuetype = Bug
- For "tasks only", use: issuetype = Task
- For "assigned to me", use: assignee = currentUser()
- For "today": use created >= -1d
- For "this week": use created >= -7d
- For "last week": use created >= -14d AND created < -7d
- Always finish with: ORDER BY updated DESC

The user may ask in English or Vietnamese.

EXAMPLES:

User: all open bugs in project DDLT
JQL: project = DDLT AND issuetype = Bug AND statusCategory != Done ORDER BY updated DESC

User: tickets assigned to me in project OPS updated this week
JQL: project = OPS AND assignee = currentUser() AND updated >= -7d ORDER BY updated DESC

User: all Done issues in project DDLT
JQL: project = DDLT AND statusCategory = Done ORDER BY updated DESC

User: liá»‡t kÃª táº¥t cáº£ bug Ä‘ang má»Ÿ cá»§a project DDLT
JQL: project = DDLT AND issuetype = Bug AND statusCategory != Done ORDER BY updated DESC

User: cÃ¡c task Ä‘Æ°á»£c assign cho tÃ´i trong tuáº§n nÃ y á»Ÿ project OPS
JQL: project = OPS AND issuetype = Task AND assignee = currentUser() AND updated >= -7d ORDER BY updated DESC

Now generate a single JQL query for this request:

User: {question}
JQL:
"""


def generate_jql_from_question(question: str, project_hint: str | None = None) -> str:
    # Náº¿u muá»‘n Ã©p project máº·c Ä‘á»‹nh, cÃ³ thá»ƒ thÃªm hint vÃ o cÃ¢u há»i
    if project_hint and project_hint not in question:
        # vÃ­ dá»¥: "in project DDLT, ..."
        question = f"trong project {project_hint}, " + question

    prompt = JQL_SYSTEM_PROMPT.format(question=question)
    raw = llama_generate(prompt)

    # cleanup Ä‘Æ¡n giáº£n
    jql = raw.strip().replace("\n", " ").replace("\r", " ").strip()

    # Má»™t sá»‘ model hay tráº£ thÃªm text kiá»ƒu "JQL: ...", ta xá»­ lÃ½ luÃ´n:
    if "JQL:" in jql:
        jql = jql.split("JQL:", 1)[1].strip()

    return jql


import org.jenkinsci.plugins.scriptsecurity.scripts.ScriptApproval

def sa = ScriptApproval.get()

sa.pendingSignatures.each {
    println "Approving signature: ${it}"
    sa.approveSignature(it)
}

sa.pendingScripts.each {
    println "Approving script: ${it.hash}"
    sa.approveScript(it.hash)
}

println "DONE - All pending scripts approved"


LDAP_ENABLED=true
LDAP_URL=ldaps://ad.company.local:636
LDAP_BASE_DN=DC=company,DC=local

# bind account Ä‘á»ƒ search user DN (khuyÃªn dÃ¹ng)
LDAP_BIND_DN=CN=svc-recert,OU=Service Accounts,DC=company,DC=local
LDAP_BIND_PASSWORD=YourBindPassword

# login báº±ng username kiá»ƒu sAMAccountName
LDAP_USER_ATTR=sAMAccountName

# filter tÃ¬m user DN
LDAP_USER_FILTER=(&(objectClass=user)({attr}={username}))

import os
from ldap3 import Server, Connection, ALL, SUBTREE

def _env(name: str, default: str = "") -> str:
    return (os.getenv(name, default) or "").strip()

def ldap_enabled() -> bool:
    return _env("LDAP_ENABLED", "false").lower() == "true"

def ldap_verify_login(username: str, password: str) -> dict | None:
    """
    Verify username/password against AD via LDAP.
    Return dict on success:
      { "username": <input username>, "user_dn": <distinguishedName>, "auth_type": "ldap" }
    Return None on failure.
    """
    if not ldap_enabled():
        return None

    ldap_url = _env("LDAP_URL")
    base_dn = _env("LDAP_BASE_DN")
    bind_dn = _env("LDAP_BIND_DN")
    bind_pw = _env("LDAP_BIND_PASSWORD")
    user_attr = _env("LDAP_USER_ATTR", "sAMAccountName")
    user_filter_tpl = _env("LDAP_USER_FILTER", "(&(objectClass=user)({attr}={username}))")

    if not ldap_url or not base_dn:
        return None

    server = Server(ldap_url, get_info=ALL)

    # 1) Bind with service account to search user's DN
    try:
        if bind_dn and bind_pw:
            search_conn = Connection(server, user=bind_dn, password=bind_pw, auto_bind=True)
        else:
            # not recommended; many ADs disable anonymous search
            search_conn = Connection(server, auto_bind=True)
    except Exception:
        return None

    try:
        search_filter = user_filter_tpl.format(attr=user_attr, username=username)
        search_conn.search(
            search_base=base_dn,
            search_filter=search_filter,
            search_scope=SUBTREE,
            attributes=["distinguishedName"],
        )
        if not search_conn.entries:
            return None

        user_dn = str(search_conn.entries[0].entry_dn)
    finally:
        search_conn.unbind()

    # 2) Bind as the user DN with provided password (this verifies login)
    try:
        user_conn = Connection(server, user=user_dn, password=password, auto_bind=True)
        user_conn.unbind()
    except Exception:
        return None

    return {"username": username, "user_dn": user_dn, "auth_type": "ldap"}



from app.auth.ldap_auth import ldap_verify_login
from app.auth.local_auth import authenticate as local_authenticate

# ...
user = None

# LDAP first
ldap_user = ldap_verify_login(username, password)
if ldap_user:
    # role sáº½ láº¥y tá»« DB á»Ÿ Step B, táº¡m thá»i cho recert_user Ä‘á»ƒ vÃ o app
    user = {"username": ldap_user["username"], "roles": ["recert_user"], "auth_type": "ldap"}

# fallback local
if not user:
    user = local_authenticate(username, password)

if not user:
    return templates.TemplateResponse("login.html", {"request": request, "error": "Login failed"})

set_session(request, user)
return RedirectResponse("/ui/form", status_code=302)







from fastapi import APIRouter, Request, Form
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.templating import Jinja2Templates

from app.auth.session import set_session, clear_session
from app.auth.ldap_auth import ldap_verify_login
from app.auth.local_auth import authenticate as local_authenticate

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")


@router.get("/login", response_class=HTMLResponse)
def login_page(request: Request):
    return templates.TemplateResponse(
        "login.html",
        {
            "request": request,
            "error": None,
        },
    )


@router.post("/login", response_class=HTMLResponse)
def login_submit(
    request: Request,
    username: str = Form(...),
    password: str = Form(...),
):
    """
    Login flow:
    1) Try LDAP (verify username/password only)
    2) Fallback to local user (break-glass)
    3) Set session
    """

    user = None

    # --- 1) LDAP authentication (login only, no role mapping) ---
    ldap_user = ldap_verify_login(username, password)
    if ldap_user:
        # Step B: roles will be loaded from DB later
        user = {
            "username": ldap_user["username"],
            "roles": ["recert_user"],   # temporary default
            "auth_type": "ldap",
        }

    # --- 2) Local fallback ---
    if not user:
        user = local_authenticate(username, password)

    # --- 3) Login failed ---
    if not user:
        return templates.TemplateResponse(
            "login.html",
            {
                "request": request,
                "error": "Invalid username or password",
            },
        )

    # --- 4) Success ---
    set_session(request, user)
    return RedirectResponse("/ui/form", status_code=302)


@router.post("/logout")
def logout(request: Request):
    clear_session(request)
    return RedirectResponse("/login", status_code=302)







