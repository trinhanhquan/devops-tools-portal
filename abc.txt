                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚     User (You)     â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Web UI (Streamlit App)  â”‚
                                    â”‚   [web/app.py]           â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚  User input
                                              â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚     ğŸ¤– Bot Logic (query_engine.py)   â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                                 â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Cypher Generator â”‚â—„â”€â”€â”€â”€â”€â”¤ If Graph Question Detected  â”‚â”€â”€â”€â”€â–¶ Generates Cypher Query     â”‚
â”‚  [llm_cypher_generator.py]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚
              â–¼                                                            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   LLM (Ollama) â”‚                                           â”‚  Knowledge Graph DB â”‚
     â”‚  llama3 model  â”‚                                           â”‚     [Neo4j]         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                               â”‚
                                                                               â–¼
                                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                             â”‚    Graph Query Result  â”‚
                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                               â”‚
                                                                               â–¼
                                                      â†â”€â”€â”€â”€â”€â”€ Result returned to Bot â”€â”€â”€â”€â”€â”€â†’

       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ If Graph route fails or   â”‚
       â”‚ Not enough info in Graph  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Vector DB (FAISS)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                   â–¼                             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
      â”‚ Semantic Search Results    â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                 â–¼                               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
      â”‚ Answer Generator (LLM)     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ [vector_llm_answer.py]     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
       â†â”€â”€â”€â”€â”€â”€ Final Answer to Bot

                     â–¼
        â†â”€â”€â”€â”€ Return Answer to UI â†â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ğŸ›  ETL BACKGROUND FLOW (once or scheduled ingest)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Jira API         â”‚
         â”‚ (Data Center/Cloud)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ ETL Script [load_jira.py]  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Insert into Vector DB  â”‚       â”‚ Insert into Graph DB (Neo4j)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         ğŸ§± Infra/Deployment Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    - Ollama runs in Docker container (port 11434)
    - Neo4j runs in container (bolt://7687, HTTP 7474)
    - Streamlit UI can run in container (port 8501)
    - Shared volume or mounted model (for Ollama)


Tools Involved
Component	Technology
Web UI	Streamlit
Bot logic	Python
LLM	LLaMA3 via Ollama
Vector DB	FAISS / Chroma
Graph DB	Neo4j
ETL	Jira API + Python
Dockerized?	Yes (optional)


import asyncio
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

async def test():
    url = "http://localhost:9000/mcp"
    async with streamablehttp_client(url) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            resp = await session.list_tools()
            print("Tools:", [t.name for t in resp.tools])

asyncio.run(test())





import asyncio
import json
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

MCP_URL = "http://localhost:9000/mcp"
ISSUE_KEY = "DDLT-111"

async def main():
    async with streamablehttp_client(MCP_URL) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            result = await session.call_tool(
                "jiraGetIssue",
                arguments={"key": ISSUE_KEY}
            )

            print("\n=== RAW CONTENT BLOCKS ===")
            for block in result.content:
                print(block)

            print("\n=== PARSED ISSUE JSON ===")
            for block in result.content:
                if hasattr(block, "text"):
                    try:
                        issue = json.loads(block.text)
                        print(json.dumps(issue, indent=2))
                    except Exception:
                        print("Not JSON block:", block.text)

asyncio.run(main())


import asyncio
import json
import requests
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

# ===========================
# CONFIG
# ===========================
MCP_URL = "http://localhost:9000/mcp"
LLAMA_URL = "http://localhost:11434/api/chat"     # Ollama endpoint

# ===========================
# CALL LLaMA (step 1)
# ===========================
def call_llama(messages):
    payload = {
        "model": "llama3.1",
        "messages": messages,
        "stream": False
    }

    r = requests.post(LLAMA_URL, json=payload)
    r.raise_for_status()
    return r.json()


# ===========================
# HANDLE TOOL CALLS
# ===========================
async def call_mcp_tool(tool_name, arguments):
    async with streamablehttp_client(MCP_URL) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool(tool_name, arguments)

            # parse MCP text blocks
            for block in result.content:
                if hasattr(block, "text"):
                    try:
                        return json.loads(block.text)
                    except:
                        return block.text

    return None


# ===========================
# MAIN AGENT LOGIC
# ===========================
async def ask_jira_agent(user_question):
    print(f"\n===== USER ASKS: {user_question} =====\n")

    messages = [
        {
            "role": "system",
            "content": (
                "You are an AI assistant with access to Jira tools. "
                "If user asks about Jira issues, ALWAYS produce tool calls in JSON like:\n"
                "{\"tool\": \"jira_get_issue\", \"arguments\": {\"key\": \"DDLT-111\"}}\n"
            )
        },
        {
            "role": "user",
            "content": user_question
        }
    ]

    # Step 1 â€” Ask LLaMA what tool to call
    llama_res = call_llama(messages)
    assistant_msg = llama_res["message"]

    print("STEP 1 â€” LLaMA response:")
    print(assistant_msg)

    # Step 2 â€” Detect tool call
    tool_calls = assistant_msg.get("tool_calls", [])

    if not tool_calls:
        print("âŒ No tool calls detected.")
        return assistant_msg["content"]

    final_answer = None

    # handle all tool calls
    for tc in tool_calls:
        tool_name = tc["tool"]
        arguments = tc["arguments"]

        print(f"\nSTEP 2 â€” Calling tool: {tool_name} with args = {arguments}")

        tool_result = await call_mcp_tool(tool_name, arguments)

        print("\nSTEP 3 â€” MCP TOOL RESULT:")
        print(json.dumps(tool_result, indent=2))

        # Step 4 â€” Feed tool result back to LLaMA for final answer
        followup_messages = [
            *messages,
            assistant_msg,
            {
                "role": "tool",
                "content": json.dumps(tool_result),
                "name": tool_name
            }
        ]

        final_llama = call_llama(followup_messages)
        final_answer = final_llama["message"]["content"]

    print("\n===== FINAL ANSWER FROM LLaMA =====")
    print(final_answer)

    return final_answer


# ===========================
# CLI EXAMPLE
# ===========================
if __name__ == "__main__":
    question = input("Ask Jira Agent: ")
    asyncio.run(ask_jira_agent(question))


